{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP12-Question1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGKoOYBHyWTQ"
      },
      "source": [
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVK2XutLzfso",
        "outputId": "9f166f25-bc36-4ffc-f78e-e49a6330911d"
      },
      "source": [
        "# Connecting to google drive for accessing the Sentiment.csv and also to save model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVW03eHLxr8-",
        "outputId": "d641cba1-ecb2-42c0-ae2e-e84a60cd223e"
      },
      "source": [
        "# Loading the Sentiment dataset\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/Deep Learning/ICP12/Sentiment.csv')\n",
        "\n",
        "# Keeping only the columns that are required. Text is input and Sentiment is the output.\n",
        "data = data[['text', 'sentiment']]\n",
        "\n",
        "print(\"Before preprocessing: \\n\")\n",
        "print(data['text'].head())\n",
        "# Applying pre-processing task\n",
        "# Converting all the characters to lower case\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "# Making using of Regular expression to remove unwanted data like @ / ' # . etc.\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "# Removing 'rt' string that is persent in all the text.\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "print(\"\\n After preprocessing: \\n\")\n",
        "print(data['text'].head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before preprocessing: \n",
            "\n",
            "0    RT @NancyLeeGrahn: How did everyone feel about...\n",
            "1    RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
            "2    RT @TJMShow: No mention of Tamir Rice and the ...\n",
            "3    RT @RobGeorge: That Carly Fiorina is trending ...\n",
            "4    RT @DanScavino: #GOPDebate w/ @realDonaldTrump...\n",
            "Name: text, dtype: object\n",
            "\n",
            " After preprocessing: \n",
            "\n",
            "0      nancyleegrahn how did everyone feel about th...\n",
            "1      scottwalker didnt catch the full gopdebate l...\n",
            "2      tjmshow no mention of tamir rice and the gop...\n",
            "3      robgeorge that carly fiorina is trending  ho...\n",
            "4      danscavino gopdebate w realdonaldtrump deliv...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQWg-cEMAlOG"
      },
      "source": [
        "# Fixing the maxFeatures as 2000 because of computation capabilities\n",
        "maxFeatures = 2000\n",
        "# Tokenizing the data\n",
        "tokenizer = Tokenizer(num_words=maxFeatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "# Converting into vector \n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "# padding zero's if the sentence is small, to make each vector of same size.\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72biHn3K3MoT",
        "outputId": "1307cc7a-5972-44d0-ab10-4fe31380dcdf"
      },
      "source": [
        "# Finding the vector length\n",
        "print(X.shape)\n",
        "senLength = X.shape[1]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13871, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehEOMrFqybjZ"
      },
      "source": [
        "# Defining the embeddingLayer dimension and Lstm nodes\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "# Function to create the model\n",
        "def createModel():\n",
        "    # Defining the model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(maxFeatures, embed_dim, input_length=X.shape[1])) # Embedding layer\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) # LSTM layer\n",
        "    model.add(Dense(3, activation='softmax')) # Fully connected layer\n",
        "    # categorical_crossentropy as the loss function since dealing with 3 categorical data.\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akNSvh07y0S2"
      },
      "source": [
        "# Converting the lables into numerical format\n",
        "labelEncoder = LabelEncoder()\n",
        "integerEncoded = labelEncoder.fit_transform(data['sentiment'])\n",
        "# Converting these numerical format into One-hot encoding\n",
        "y = to_categorical(integerEncoded)\n",
        "# Splitting the data into training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPT43gSPzCNO",
        "outputId": "997ef8eb-4dc0-4b48-9724-69d1da7be1f2"
      },
      "source": [
        "# Model summary\n",
        "model = createModel()\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 28, 128)           256000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 591       \n",
            "=================================================================\n",
            "Total params: 511,391\n",
            "Trainable params: 511,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOz3CQUhy-IK",
        "outputId": "907fef64-2e54-4d25-eb25-1a1a202b11cd"
      },
      "source": [
        "# Training the model for 10 epochs with batch size of 32.\n",
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, epochs=10, batch_size=batch_size, verbose=2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "291/291 - 29s - loss: 0.6817 - accuracy: 0.7099\n",
            "Epoch 2/10\n",
            "291/291 - 29s - loss: 0.6140 - accuracy: 0.7407\n",
            "Epoch 3/10\n",
            "291/291 - 29s - loss: 0.5699 - accuracy: 0.7654\n",
            "Epoch 4/10\n",
            "291/291 - 29s - loss: 0.5182 - accuracy: 0.7875\n",
            "Epoch 5/10\n",
            "291/291 - 29s - loss: 0.4781 - accuracy: 0.8035\n",
            "Epoch 6/10\n",
            "291/291 - 30s - loss: 0.4416 - accuracy: 0.8220\n",
            "Epoch 7/10\n",
            "291/291 - 29s - loss: 0.4099 - accuracy: 0.8342\n",
            "Epoch 8/10\n",
            "291/291 - 30s - loss: 0.3759 - accuracy: 0.8470\n",
            "Epoch 9/10\n",
            "291/291 - 29s - loss: 0.3497 - accuracy: 0.8541\n",
            "Epoch 10/10\n",
            "291/291 - 29s - loss: 0.3272 - accuracy: 0.8670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f135c4d82d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5urV1mbnzFyt",
        "outputId": "601513e7-7e6f-4e03-9f06-65a66af987eb"
      },
      "source": [
        "# Model Prediction on test dataset\n",
        "score, acc = model.evaluate(X_test, Y_test, verbose=2, batch_size=batch_size)\n",
        "print(\"Score: \",score,\"  Accuracy: \",acc)\n",
        "print(\"Metrics\",model.metrics_names)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "144/144 - 1s - loss: 1.2811 - accuracy: 0.6549\n",
            "Score:  1.2811222076416016   Accuracy:  0.6548711061477661\n",
            "Metrics ['loss', 'accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2HzmwbX04RL"
      },
      "source": [
        "# Saving the Model\n",
        "model.save(\"/content/gdrive/MyDrive/Deep Learning/ICP12/model.h5\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puXNkCPP1Blr",
        "outputId": "2ad9b8b3-c4a9-4eae-ee35-d7054f3fac9b"
      },
      "source": [
        "# Loading the saved model\n",
        "savedModel = load_model('/content/gdrive/MyDrive/Deep Learning/ICP12/model.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoJf63DtFAaQ"
      },
      "source": [
        "Model prediction on the given Sample text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJKVpEc71Hn9",
        "outputId": "e293ce14-db7c-474d-9808-286f22d89405"
      },
      "source": [
        "# preprocessing the given data\n",
        "sentence = [\"A lot of good things are happening. We are respected again throughout the world, and that's a great thing\"]\n",
        "sentence[0] = sentence[0].lower()\n",
        "sentence[0] = re.sub('[^a-zA-z0-9\\s]', '', sentence[0])\n",
        "# Tokenizing the data and converting to the vector format\n",
        "sentence = tokenizer.texts_to_sequences(sentence)\n",
        "# Adding the extra zero's to much the length of senLength\n",
        "sentence = pad_sequences(sentence, maxlen=senLength)\n",
        "\n",
        "print(sentence)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   7 445   5 146 292  35\n",
            "   30  35 371   2 349   8 262   7 153 265]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNAgeuQm3Uvx",
        "outputId": "937592c1-ca60-408c-b5b2-dde85d4a7b93"
      },
      "source": [
        "# Model prediction\n",
        "prob = savedModel.predict(sentence,batch_size=1)\n",
        "print(\"The probabilty of sentence is: \",prob)\n",
        "pred = np.argmax(prob)\n",
        "print(\"The model predicted the given sentence as: \",labelEncoder.classes_[pred])\n",
        "# The model is 93.37% certain the given sentence is Positive."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The probabilty of sentence is:  [[0.06382039 0.00245245 0.9337271 ]]\n",
            "The model predicted the given sentence as:  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH1mOIm34nf8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}